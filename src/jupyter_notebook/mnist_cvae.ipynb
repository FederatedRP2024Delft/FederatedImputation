{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:28:12.594616600Z",
     "start_time": "2024-05-17T09:28:02.932395500Z"
    }
   },
   "source": [
    "import torchvision\n",
    "\n",
    "from torch import cuda, device, Tensor, save\n",
    "from src.plots import plot_vae_training_result, plot_image\n",
    "from src.vae.mnist_vae import ConditionalVae\n",
    "\n",
    "device = device('cuda' if cuda.is_available() else 'cpu')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:28:12.888525800Z",
     "start_time": "2024-05-17T09:28:12.607434500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_data = torchvision.datasets.MNIST(root='../data/MNIST_train', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "testing_data = torchvision.datasets.MNIST(root='../data/MNIST_test', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "print(training_data)\n",
    "print(testing_data)\n",
    "\n",
    "input = training_data.data[:60000] / 255.0    # normalizing necessary to make pixels in [0, 1] range for FID\n",
    "labels = training_data.targets[:60000]"
   ],
   "id": "16ee1ea9b79842dc",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\n",
    "np.save(\"actu_train\", training_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T09:29:43.723641400Z",
     "start_time": "2024-05-17T09:29:30.607763500Z"
    }
   },
   "id": "241d7c923d5b3d97",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T15:47:04.286150100Z",
     "start_time": "2024-05-16T15:46:45.350867800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train VAE\n",
    "vae = ConditionalVae(dim_encoding=3).to(device)\n",
    "\n",
    "# try with model sigma\n",
    "vae_model, vae_loss_li, kl_loss_li = vae.train_model(\n",
    "    training_data=training_data,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "print(vae_loss_li)\n",
    "# save(vae, \"./model/50_epoch_cvae\")"
   ],
   "id": "415ddc3205ecd96b",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T19:04:30.238838Z",
     "start_time": "2024-05-15T19:04:29.871448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# move tensors to cpu before converting to np array\n",
    "np_kl_loss_li = []\n",
    "\n",
    "for output in kl_loss_li:\n",
    "    if isinstance(output, Tensor):\n",
    "        np_kl_loss_li.append(output.cpu().detach().numpy())\n",
    "\n",
    "# plot results\n",
    "plot_vae_training_result(\n",
    "    input=input,\n",
    "    labels=labels,\n",
    "    vae_model=vae_model,\n",
    "    vae_loss_li=vae_loss_li,\n",
    "    kl_loss_li=np_kl_loss_li\n",
    ")"
   ],
   "id": "9ce187083360ae0e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T19:04:44.202102Z",
     "start_time": "2024-05-15T19:04:44.113312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images = vae.generate_data(n_samples=5, target_label=0)\n",
    "plot_image(images)"
   ],
   "id": "8c38d9f63fe4cdfc",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:52:58.572754Z",
     "start_time": "2024-05-14T13:52:58.365797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from src.sampling import split_dirichlet\n",
    "# \n",
    "# # generate imbalanced data set for comparison of distribution of input vs distribution of generated images\n",
    "# training_data = torchvision.datasets.MNIST(root='../data/MNIST_train', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "# \n",
    "# input = training_data.data[:60000]\n",
    "# labels = training_data.targets[:60000]\n",
    "# \n",
    "# users_data = split_dirichlet(dataset=training_data, num_users=4, is_cfar=False, beta=0.5)\n",
    "# \n",
    "# total_input = []\n",
    "# total_labels = []\n",
    "# total_counts = []\n",
    "# for user_idx in users_data:\n",
    "#     images = []\n",
    "#     outputs = []\n",
    "#     counts = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# \n",
    "#     for data_idx in users_data[user_idx]:\n",
    "#         image = input[int(data_idx)]\n",
    "#         images.append(image)\n",
    "#         label = labels[int(data_idx)]\n",
    "#         outputs.append(label)\n",
    "#         counts[label] +=1\n",
    "#     total_input.append(images)\n",
    "#     total_labels.append(outputs)\n",
    "#     total_counts.append(counts)\n",
    "# \n",
    "# user_idx = 0\n",
    "# sample_input = total_input[user_idx]\n",
    "# sample_label = total_labels[user_idx]\n",
    "# \n",
    "# input_tensor = torch.stack(sample_input)\n",
    "# label_tensor = torch.stack(sample_label)\n",
    "# \n",
    "# plot_image_label_two (input_tensor.cpu().detach().numpy(), label_tensor.cpu().detach().numpy())\n",
    "# \n",
    "# assert input_tensor.shape[0] == label_tensor.shape[0]\n",
    "# \n",
    "# training_data.data = input_tensor\n",
    "# training_data.targets = label_tensor\n",
    "# \n",
    "# assert training_data.data.shape == input_tensor.shape\n",
    "# assert training_data.targets.shape == label_tensor.shape\n",
    "# \n",
    "# \n",
    "# # Train VAE on imbalanced dataset\n",
    "# vae_imbalanced = VaeAutoencoder(dim_encoding=2)\n",
    "# \n",
    "# _, _, _ = vae_imbalanced.train_model(\n",
    "#     training_data,\n",
    "#     batch_size=50,\n",
    "#     beta=1000,\n",
    "#     epochs=20\n",
    "# )\n",
    "# \n",
    "# gen_image = vae_imbalanced.generate_data(n_samples=sum(total_counts[user_idx]))\n",
    "# gen_output = classifier.generate_labels(gen_image)\n",
    "# gen_counts = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# for tensor_label in gen_output:\n",
    "#     gen_counts[tensor_label[0]]+=1\n",
    "# \n",
    "# # plot generated data\n",
    "# plot_image_label_two(gen_image.cpu().detach().numpy(), gen_output.cpu().detach().numpy())\n",
    "# \n",
    "# print(\"Input counts: \", total_counts[user_idx])\n",
    "# print(\"Generated counts: \", gen_counts)"
   ],
   "id": "9120a59d9016136e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# x = vae.generate_data(n_samples=10000)\n",
    "# print(\"Number of images: \", x.shape[0])\n",
    "# \n",
    "# labels = classifier.generate_labels(x)\n",
    "# print(\"Labels: \", labels.shape)"
   ],
   "id": "1296469b8fed46fe",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Determine FID\n",
    "# # generate 500 images\n",
    "# syn_input, _ = vae.generate_data(n_samples=500)\n",
    "# input = input[:500]\n",
    "# \n",
    "# input_rgb = input.view(-1, 1, 28, 28).repeat(1, 3, 1, 1)\n",
    "# syn_input_rgb = syn_input.view(-1, 1, 28, 28).repeat(1, 3, 1, 1)\n",
    "# \n",
    "# # compute FID score\n",
    "# fid_score = frechet_inception_distance(input_rgb, syn_input_rgb)\n",
    "# print(\"Frechet Inception Distance: \", fid_score)"
   ],
   "id": "8facfceca09d32d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.vae.mnist_vae import ConditionalVae\n",
    "from src.plots import plot_vae_training_result, plot_image\n",
    "cvae = ConditionalVae(dim_encoding=3)\n",
    "checkpoint = torch.load('C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\vae_data\\\\models\\\\0_cvae_0.1.pth')\n",
    "cvae.load_state_dict(checkpoint)\n",
    "plot_image(cvae.generate_data(n_samples=5, target_label=9))"
   ],
   "id": "e096decd0448907b",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cvae = ConditionalVae(dim_encoding=3)\n",
    "checkpoint = torch.load('C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\vae_data\\\\models\\\\0_cvae_1.0.pth')\n",
    "cvae.load_state_dict(checkpoint)\n",
    "plot_image(cvae.generate_data(n_samples=5, target_label=1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf5d731102f316c",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import pickle\n",
    "path = f\"C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\save\\\\objects\\\\fedimputed_cvae_0_0.1_mnist_exq_15_C[1.0]_iid[2]_E[1]_B[10].pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    a = pickle.load(f)\n",
    "path = f\"C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\save\\\\objects\\\\fedimputed_cvae_100_0.1_mnist_exq_15_C[1.0]_iid[2]_E[1]_B[10].pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    b = pickle.load(f)\n",
    "path = f\"C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\save\\\\objects\\\\fedimputed_cvae_750_0.1_mnist_exq_15_C[1.0]_iid[2]_E[1]_B[10].pkl\"\n",
    "with open(path, 'rb') as f:\n",
    "    c = pickle.load(f)\n",
    "plt.plot(np.mean(a[1], axis=0), label = f\"no uagmentation\")\n",
    "plt.plot(np.mean(b[1], axis=0), label = f\"100 uagmentation\")\n",
    "plt.plot(np.mean(c[1], axis=0), label = f\"750 uagmentation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1cc588ddfd7248b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from src.impute import impute_cvae_naive\n",
    "\n",
    "# print(impute_cvae_naive(k=100, trained_cvae=cvae, initial_dataset=training_data)[0][0].shape)\n",
    "generated_cvae_dataset = impute_cvae_naive(k=100, trained_cvae=cvae, initial_dataset=training_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e0ab6b3609f8c2",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "len(generated_cvae_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0bd2b4cc1af1600",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "generated_cvae_dataset[60099][0].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "755ad4d0a5cdbc53",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cvae.generate_data(n_samples=1, target_label=1).shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a92a58deb80e295d",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from src.impute import impute_naive\n",
    "from src.vae.mnist_vae import VaeAutoencoderClassifier\n",
    "\n",
    "trained_vae = VaeAutoencoderClassifier(dim_encoding=2)\n",
    "trained_vae.load_state_dict(torch.load(\"C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\vae_data\"\n",
    "                                       f\"\\\\models\\\\vae_{0.1}.pth\"))\n",
    "\n",
    "generated_train_dataset = impute_naive(k=100, trained_vae=trained_vae, initial_dataset=training_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d22ce6eec76823fb",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "len(generated_train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74f24752816a5087",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "generated_train_dataset[60099][0].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f57f4f2f69ab1e72",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from src.models import ExquisiteNetV1\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "model = ExquisiteNetV1(class_num=10, img_channels=1)\n",
    "with torch.no_grad():\n",
    "    output = model(torch.stack([test_img for test_img, _ in testing_data]))\n",
    "predictions = torch.argmax(output, dim=1).cpu().numpy()  # Get class from model's prediction\n",
    "test_labels = np.array([test_label for _, test_label in testing_data])\n",
    "test_labels = np.array([label.item() for label in test_labels])\n",
    "# Compute f1 score. Use 'micro' to aggregate the contributions of all classes when calculating the average\n",
    "f1 = f1_score(test_labels, predictions, average='micro')\n",
    "print(\"F1 Score: \", f1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8528625509a8b073",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from src.models import ExquisiteNetV1\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "model = ExquisiteNetV1(class_num=10, img_channels=1)\n",
    "checkpoint = torch.load('C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\vae_data\\\\models\\\\0_exq_0.1_cvae.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "with torch.no_grad():\n",
    "    output = model(torch.stack([test_img for test_img, _ in testing_data]))\n",
    "predictions = torch.argmax(output, dim=1).cpu().numpy()  # Get class from model's prediction\n",
    "test_labels = np.array([test_label for _, test_label in testing_data])\n",
    "test_labels = np.array([label.item() for label in test_labels])\n",
    "# Compute f1 score. Use 'micro' to aggregate the contributions of all classes when calculating the average\n",
    "f1 = f1_score(test_labels, predictions, average='macro')\n",
    "print(\"F1 Score: \", f1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4656bf93a47f1111",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from src.models import ExquisiteNetV1\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "model = ExquisiteNetV1(class_num=10, img_channels=1)\n",
    "checkpoint = torch.load('C:\\\\Users\\\\LohithSai\\\\Desktop\\\\FederatedImputation\\\\vae_data\\\\models\\\\0_exq_0.9_cvae.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "with torch.no_grad():\n",
    "    output = model(torch.stack([test_img for test_img, _ in testing_data]))\n",
    "predictions = torch.argmax(output, dim=1).cpu().numpy()  # Get class from model's prediction\n",
    "test_labels = np.array([test_label for _, test_label in testing_data])\n",
    "test_labels = np.array([label.item() for label in test_labels])\n",
    "# Compute f1 score. Use 'micro' to aggregate the contributions of all classes when calculating the average\n",
    "f1 = f1_score(test_labels, predictions, average='micro')\n",
    "print(\"F1 Score: \", f1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae6d28e55ad4b98b",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "18694bb44e0adb22",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
